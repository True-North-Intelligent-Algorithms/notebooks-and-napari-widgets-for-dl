{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data with SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellpose import models, io\n",
    "import os\n",
    "import numpy as np\n",
    "import napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found class  PytorchSemanticModel\n",
      "found class  CellPoseInstanceModel\n",
      "creating new log file\n",
      "2024-10-23 07:46:47,470 [INFO] WRITING LOG OUTPUT TO C:\\Users\\bnort\\.cellpose\\run.log\n",
      "2024-10-23 07:46:47,471 [INFO] \n",
      "cellpose version: \t3.0.9 \n",
      "platform:       \twin32 \n",
      "python version: \t3.10.14 \n",
      "torch version:  \t2.2.2+cu118\n",
      "found class  MobileSAMModel\n",
      "found class  YoloSAMModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x1024 17 objects, 45.9ms\n",
      "Speed: 5.0ms preprocess, 45.9ms inference, 4.9ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x768 6 objects, 172.5ms\n",
      "Speed: 8.0ms preprocess, 172.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x512 63 objects, 170.6ms\n",
      "Speed: 3.0ms preprocess, 170.6ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x768 13 objects, 42.9ms\n",
      "Speed: 5.0ms preprocess, 42.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x960 39 objects, 184.5ms\n",
      "Speed: 5.0ms preprocess, 184.5ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x768 117 objects, 41.9ms\n",
      "Speed: 4.1ms preprocess, 41.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 704x1024 16 objects, 191.5ms\n",
      "Speed: 4.0ms preprocess, 191.5ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x768 26 objects, 39.9ms\n",
      "Speed: 4.0ms preprocess, 39.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x768 49 objects, 40.9ms\n",
      "Speed: 4.9ms preprocess, 40.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 768x1024 90 objects, 160.6ms\n",
      "Speed: 5.0ms preprocess, 160.6ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x768 39 objects, 39.9ms\n",
      "Speed: 5.0ms preprocess, 39.9ms inference, 2.9ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 768x1024 4 objects, 41.9ms\n",
      "Speed: 4.9ms preprocess, 41.9ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x704 18 objects, 178.5ms\n",
      "Speed: 5.0ms preprocess, 178.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 768x1024 16 objects, 44.0ms\n",
      "Speed: 4.9ms preprocess, 44.0ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 672x1024 60 objects, 168.5ms\n",
      "Speed: 5.0ms preprocess, 168.5ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x928 18 objects, 168.5ms\n",
      "Speed: 8.0ms preprocess, 168.5ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x1024 13 objects, 53.9ms\n",
      "Speed: 12.0ms preprocess, 53.9ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x768 45 objects, 40.9ms\n",
      "Speed: 5.0ms preprocess, 40.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x768 189 objects, 44.9ms\n",
      "Speed: 7.0ms preprocess, 44.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 608x1024 51 objects, 174.4ms\n",
      "Speed: 4.0ms preprocess, 174.4ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x768 20 objects, 39.0ms\n",
      "Speed: 9.9ms preprocess, 39.0ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x1024 33 objects, 50.9ms\n",
      "Speed: 6.0ms preprocess, 50.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 864x1024 15 objects, 181.6ms\n",
      "Speed: 5.0ms preprocess, 181.6ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x768 37 objects, 41.0ms\n",
      "Speed: 5.0ms preprocess, 41.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x768 9 objects, 40.5ms\n",
      "Speed: 4.1ms preprocess, 40.5ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x704 108 objects, 36.9ms\n",
      "Speed: 4.0ms preprocess, 36.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x768 36 objects, 39.0ms\n",
      "Speed: 5.0ms preprocess, 39.0ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 768x1024 176 objects, 46.0ms\n",
      "Speed: 4.0ms preprocess, 46.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIT checkpoint loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from napari_easy_augment_batch_dl import easy_augment_batch_dl\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "\n",
    "batch_dl = easy_augment_batch_dl.NapariEasyAugmentBatchDL(viewer, label_only = False)\n",
    "\n",
    "viewer.window.add_dock_widget(\n",
    "    batch_dl\n",
    ")\n",
    "\n",
    "data_path = r'C:\\Users\\bnort\\work\\ImageJ2022\\tnia\\notebooks-and-napari-widgets-for-dl\\data'\n",
    "parent_path = os.path.join(data_path, 'ladybugs_series')\n",
    "\n",
    "\n",
    "#mod = models.Cellpose(gpu=True, model_type=\"cyto3\")\n",
    "\n",
    "batch_dl.load_image_directory(parent_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 65535)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_path =r'D:\\images\\tnia-python-images\\\\imagesc\\\\2024_10_07_cellpose_multi_nuclear'\n",
    "\n",
    "test = io.imread(os.path.join(parent_path, 'Empty_02 - Copy.tif')).astype('uint16')\n",
    "\n",
    "test.min(), test.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 1024, 1024), dtype('uint16'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape, test.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = io.imread(r'D:\\images\\tnia-python-images\\imagesc\\2024_10_03_cellpose_ladybugs\\5622668_6951835.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1024, 1024)\n",
      "(1024, 1024, 2)\n",
      "(1024, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)\n",
    "test2 = test.transpose((1, 2, 0))\n",
    "print(test2.shape)\n",
    "test3 = np.concatenate([test2, np.zeros(test3.shape[:2])[:, :, None]], axis=2)\n",
    "print(test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'test3' at 0x1f3ff3f2b00>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import napari\n",
    "viewer = napari.Viewer()\n",
    "\n",
    "viewer.add_image(test)\n",
    "viewer.add_image(test2)\n",
    "viewer.add_image(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move axis so that the channel is the last axis\n",
    "test3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add empty channel\n",
    "test4 = np.concatenate([test3, np.zeros(test3.shape[:2])[:, :, None]], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test4.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from napari_easy_augment_batch_dl.mobile_sam_model import MobileSAMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_and_SAM3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
